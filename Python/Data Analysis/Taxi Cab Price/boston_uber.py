# -*- coding: utf-8 -*-
"""Boston_Uber.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UCWWrzvghadISBCZ58mEw5oZOSr9IXAs
"""

from google.colab import drive
drive.mount('/content/drive')

# import os

# path = '/content/drive/MyDrive/Deep Learning/'
# os.chdir(path)

# Extract dataset
!unzip Boston_Uber.zip

import os
os.chdir('/content/drive/MyDrive/Deep Learning/Boston_Uber/')

!unzip rideshare_kaggle.csv.zip

import pandas as pd
import numpy as np
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import calendar

data = pd.read_csv('rideshare_kaggle.csv')

data.head()

copy = data.copy()

# Handle unwanted data:
print(data.isnull().sum())
data = data.dropna()

ex_cols = ['apparentTemperature','apparentTemperatureMaxTime','apparentTemperatureMax','apparentTemperatureMinTime',
           'apparentTemperatureMin','temperatureMaxTime','temperatureMinTime','apparentTemperatureHigh',
           'apparentTemperatureLowTime','apparentTemperatureHighTime','apparentTemperatureHigh','temperatureLowTime',
           'temperatureHighTime','precipIntensity','humidity','windGust','windSpeed','dewPoint','precipIntensityMax','cloudCover',
           'moonPhase','windGustTime','visibility','uvIndex','windBearing','visibility.1','ozone','sunriseTime',
           'sunsetTime','uvIndexTime']

data=data.drop(ex_cols,axis=1)

data['datetime'] = pd.to_datetime(data['datetime'], format = "%Y/%m/%d %H:%M:%S")

# See at what time of the day the user rides an Uber the most?
hours = data['hour'].value_counts()
hours.plot(kind = 'bar', color = 'red', figsize = (10,5))
plt.xlabel('Hours')
plt.ylabel('Frequency')
plt.title('Number of trip VS hours')

day_week = [calendar.day_name[x.dayofweek] for x in data['datetime']]
day_week = pd.Series(day_week)

# Look number of trip in days of week
days=day_week.value_counts()
days.plot(kind = 'barh', color = 'red', figsize = (10,5))
plt.xlabel('Freq')
plt.ylabel('Day')
plt.title('Number of trip VS days')

# Look number of trip in month
months = data['month'].value_counts()
months.plot(kind = 'barh', color = 'blue', figsize = (10,5))
plt.xlabel('Months')
plt.ylabel('Frequency')
plt.title('Number of trip VS months')
plt.show()

day = data['day'][data['month']==12].value_counts()
day.plot(kind = 'bar', color = 'darkgrey', figsize = (10,5))
plt.xlabel('Days of December')
plt.ylabel('Frequency')
plt.title("Number of trip VS December's day")
plt.show()

pick_up = data['source'].value_counts().nlargest(10)
pick_up.plot(kind = 'barh', color = 'lightblue', figsize = (10,5))
plt.xlabel('Frequency')
plt.ylabel('Pick up point')
plt.title("Pick up point VS Freq")
plt.show()

dest = data['destination'].value_counts().nlargest(10)
dest.plot(kind = 'barh', color = 'lightgreen', figsize = (10,5))
plt.xlabel('Frequency')
plt.ylabel('Destination')
plt.title("Destination VS Freq")
plt.show()

# see which types of Uber cabs do people prefer in Boston
cabs = data['name'].value_counts()
cabs.plot(kind = 'barh', color = 'pink', figsize = (10,5))
plt.xlabel('Frequency')
plt.ylabel('Cabs')
plt.title("Freq")
plt.show()

# Predictive Analysis for Uber Price Prediction using ML

# Bring all the categorical data to the numeric format using label encoding:
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

data['id'] = label_encoder.fit_transform(data['id'])
data['datetime'] = label_encoder.fit_transform(data['datetime'])
data['timezone'] = label_encoder.fit_transform(data['timezone'])
data['destination'] = label_encoder.fit_transform(data['destination'])
data['product_id'] = label_encoder.fit_transform(data['product_id'])
data['short_summary'] = label_encoder.fit_transform(data['short_summary'])
data['long_summary'] = label_encoder.fit_transform(data['long_summary'])
data['name'] = label_encoder.fit_transform(data['name'])

print("Class mapping of Name: \n")
for i,item in enumerate(label_encoder.classes_):
    print(item,"==>",i)

# Label for pick up point:
data['source'] = label_encoder.fit_transform(data['source'])
print("Class mapping of Source: \n")
for i,item in enumerate(label_encoder.classes_):
    print(item,"==>",i)

# Label for current weather:
data['icon'] = label_encoder.fit_transform(data['icon'])
print("Class mapping of Weather: \n")
for i,item in enumerate(label_encoder.classes_):
    print(item,"==>",i)

# Label for cab type:
data['cab_type'] = label_encoder.fit_transform(data['cab_type'])
print("Class mapping of Cab type: \n")
for i,item in enumerate(label_encoder.classes_):
    print(item,"==>",i)

# Since we are only predicting the prices for Uber, our dataset will contain ~ 400000 records
len(data[data['cab_type']==1])

# Train-test split with the price col as target:
X = data[data['cab_type']==1].drop(['price','cab_type'], axis=1)
y = data[data['cab_type']==1]['price']

X.head()

y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

# look at what the typical prices are for the rides
plt.hist(y,bins=20,color='orange')
plt.show()

# Train and compare the performance of four ML models: linear regression, decision tree, random forest, and gradient boosting.
# Feature selection technique like RFE would be helpful for optimal analysis

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import ensemble

# Define function to train and test all 4 models:
def train_test_models(X_train, y_train, X_test, y_test):
    print("Linear Regression:")
    lr=LinearRegression()
    lr.fit(X_train,y_train)
    print(lr.score(X_test, y_test))
    
    print("Decision Tree:")
    dt=DecisionTreeRegressor(random_state=0, max_depth=10)
    dt.fit(X_train,y_train)
    print(dt.score(X_test, y_test))
    
    print("RandomForestRegressor:")
    rf=RandomForestRegressor(n_estimators=20,random_state=0)
    rf.fit(X_train,y_train)
    print(rf.score(X_test, y_test))
    
    print("Gradient Boosting:")
    gb=ensemble.GradientBoostingRegressor(n_estimators=200,max_depth=5)
    gb.fit(X_train,y_train)
    print(gb.score(X_test, y_test))
    
    return [lr,dt,rf,gb]

# Define a function that runs RFE and extracts the most important features for each type of model trained:
def features_eliminate(trained_model, X, y, n_features=40):
    rfe=RFE(trained_model, n_features_to_select = n_features)
    rfe.fit(X,y)
    X_new=X[X.columns[rfe.support_]]
    X_train, X_test, y_train, y_test = train_test_split(X_new,y,
                                                        test_size=0.2,
                                                        random_state=0)
    new_fit=trained_model.fit(X_train,y_train)
    print(new_fit.score(X_test,y_test))

model_list = train_test_models(X_train, y_train, X_test, y_test)

# find the models perform marginally better with fewer features
n_features = [5,10,15,20]

for model in model_list:
  for nf in n_features:
    print(f"{model}-->{nf} features")
    features_eliminate(model, X, y, nf)