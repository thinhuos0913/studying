# -*- coding: utf-8 -*-
"""Uber_Lyft_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hafXSf-fvjo_jiXDg1JRFNANufyLM9I9
"""

from google.colab import drive
drive.mount('/content/drive')

import os

os.chdir('/content/drive/MyDrive/Deep Learning/Boston_Uber/')

"""**Exploratory Data Analysis and using Regression algorithms to predict Uber-Lyft price based on Uber dataset.**"""

import pandas as pd
import numpy as np
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import calendar

pd.options.display.max_rows = None
pd.options.display.max_columns = None

data = pd.read_csv("rideshare_kaggle.csv")

data.head()

data.shape

data.info()

data['datetime']=pd.to_datetime(data['datetime'])

"""1. Data cleaning"""

data.isnull().sum().sum()

data.dropna(axis=0,inplace=True)

data.isnull().sum().sum()

data['visibility'].head()

data = data.drop(['visibility.1'],axis=1)

data.shape

"""2. EDA and Visualization"""

# Define plot function
def plot_bar(column, bar_type, color):
  col_name = data[column].value_counts()
  col_name.plot(kind = bar_type, color = color , figsize = (10,5))
  plt.xlabel(f"{column}")
  plt.ylabel('Frequency')
  plt.title(f'Number of trip VS {column}')

# Month
plot_bar('month','bar','red')

# Day
day_week = [calendar.day_name[x.dayofweek] for x in data['datetime']]
day_week = pd.Series(day_week)

day_week.unique()

# Look number of trip in days of week
days=day_week.value_counts()
days.plot(kind = 'barh', color = 'red', figsize = (10,5))
plt.xlabel('Freq')
plt.ylabel('Day of week')
plt.title('Number of trip VS day of week')

# Hour
plot_bar('hour','bar','yellow')

# Source & Destination analysis
plot_bar('source', 'barh', 'blue')

plot_bar('destination', 'barh', 'lightblue')

!pip install -U kaleido

import plotly.express as px
s_d =data.groupby(by=["source","destination"]).size().reset_index(name="counts")
# s_d.head()
fig2 = px.bar(data_frame=s_d, x="source", y="counts", color="destination", barmode="group")
fig2.show(renderer='png')

# Cab type analysis
plot_bar('cab_type','barh','grey')

df2 =data.groupby(by=["day","cab_type"]).size().reset_index(name="counts")
fig2 = px.bar(data_frame=df2, x="day", y="counts", color="cab_type", barmode="group")
fig2.show(renderer='png')

df3 =data.groupby(["hour","cab_type"]).size().reset_index(name="counts")
fig3 = px.bar(data_frame=df3, x="hour", y="counts", color="cab_type", barmode="group")
fig3.show(renderer='png')

# Price Analysis
pd.set_option('display.max_rows', 72)
data.groupby(by=["source","destination"]).price.agg(["mean"])

# Max price
print('Maximum price in our data :',data.price.max())
data[data['price']==data.price.max()]

"""The 'Financial District - Fenway' route (by Lyft) costs 97.5 dollars, which is our maximum price data"""

data_group = data.groupby(by=["source","destination"]).price.agg(["mean"]).reset_index()
data_group[(data_group['source']=='Financial District')& (data_group['destination']=='Fenway')]

"""But the mean of the price data of that route is 23.4 dollars, which is far from our maximum price data (97.5 dollars) => it's possible an outlier and we can remove it."""

data = data.loc[data['price']!=data.price.max()]

data.head()

"""3. Data Processing & Feature Engineering"""

# Removing Unnecessary Features
data = data.drop(['id','timestamp','datetime','long_summary','apparentTemperatureHighTime','apparentTemperatureLowTime',
                  'apparentTemperatureLowTime','windGustTime','sunriseTime','sunsetTime','uvIndexTime','temperatureMinTime',
                 'temperatureMaxTime','apparentTemperatureMinTime','temperatureLowTime','apparentTemperatureMaxTime'],axis=1)

data.shape

# Check the correlation of temperature related features with target feature (Price)
temp_cols= ['temperature','apparentTemperature','temperatureHigh','temperatureLow','apparentTemperatureHigh',
            'apparentTemperatureLow','temperatureMin','temperatureHighTime','temperatureMax','apparentTemperatureMin',
            'apparentTemperatureMax','price']

df_temp = data[temp_cols]
df_temp.head()

plt.figure(figsize=(15,20))
sns.heatmap(df_temp.corr(),annot=True)

"""All temperature related features have weak correlation with our target feature which is price => remove them will not much impact regression model"""

data = data.drop(['temperature','apparentTemperature','temperatureHigh','temperatureLow','apparentTemperatureHigh',
                'apparentTemperatureLow','temperatureMin','temperatureHighTime','temperatureMax','apparentTemperatureMin','apparentTemperatureMax'],axis=1)
data.shape

climate_column = ['precipIntensity', 'precipProbability', 'humidity', 'windSpeed',
       'windGust', 'visibility', 'dewPoint', 'pressure', 'windBearing',
       'cloudCover', 'uvIndex', 'ozone', 'moonPhase',
       'precipIntensityMax','price']
df_clim = data[climate_column]
df_clim.head()

plt.figure(figsize=(15,20))
sns.heatmap(df_clim.corr(),annot=True)

"""Once again, due to weak correlation with price => removing all of climate features will not make any impact to our regression model"""

data = data.drop(['precipIntensity', 'precipProbability', 'humidity', 'windSpeed',
       'windGust', 'visibility', 'dewPoint', 'pressure', 'windBearing',
       'cloudCover', 'uvIndex', 'ozone', 'moonPhase',
       'precipIntensityMax'],axis=1)
data.shape

# Check categorical value in our dataset features
category_col = data.select_dtypes(include=['object','category']).columns.tolist()
for column in data[category_col]:
    print(f'{column} : {data[column].unique()}')
    print()

data = data.drop(['timezone','product_id'],axis=1)

data.shape

# check the correlation of categorical features with target feature (price)
new_cat = ['source','destination','cab_type','name','short_summary','icon','price']

df_cat = data[new_cat]
df_cat.head()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

df_cat_encode= df_cat.copy()
for col in df_cat_encode.select_dtypes(include='O').columns:
    df_cat_encode[col]=le.fit_transform(df_cat_encode[col])

plt.figure(figsize=(15,20))
sns.heatmap(df_cat_encode.corr(),annot=True)

data = data.drop(['source','destination','short_summary','icon'],axis=1)
data.head()

data.shape

data = data.drop(['hour','day','month','latitude','longitude'],axis=1)
data.head()

data.columns



"""4. Regression Model"""

# One-hot encoding data
def one_hot_encoder(data,feature,keep_first=True):

    one_hot_cols = pd.get_dummies(data[feature])
    
    for col in one_hot_cols.columns:
        one_hot_cols.rename({col:f'{feature}_'+col},axis=1,inplace=True)
    
    new_data = pd.concat([data,one_hot_cols],axis=1)
    new_data.drop(feature,axis=1,inplace=True)
    
    if keep_first == False:
        new_data=new_data.iloc[:,1:]
    
    return new_data

data_onehot = data.copy()
for col in data_onehot.select_dtypes(include='O').columns:
    new_df_onehot = one_hot_encoder(data_onehot,col)
    
new_df_onehot.head()

new_df_onehot = new_df_onehot.drop(['cab_type'],axis=1)
new_df_onehot.head()

# Split dataset
from sklearn.model_selection import train_test_split
X = new_df_onehot.drop(columns=['price'],axis=1).values
y = new_df_onehot['price'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

from sklearn.linear_model import LinearRegression
reg = LinearRegression()

# Fit to data training
model = reg.fit(X_train,y_train)
y_pred=model.predict(X_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test,y_pred)
rmse = np.sqrt(mse)
print(mse)
print(rmse)

"""Improve performance with other regression models which could give better results."""

# Finding Best Models with best configuration with GridSearch CV
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV,ShuffleSplit

def find_best_model(X,y):
    algos = {
        'linear_regression' : {
            'model': LinearRegression(),
            'params': {
                'normalize': [True, False]
            }
        },
        'lasso': {
            'model': Lasso(),
            'params': {
                'alpha': [1,2],
                'selection': ['random', 'cyclic']
            }
        },
        'decision_tree': {
            'model': DecisionTreeRegressor(),
            'params': {
                'criterion' : ['mse','friedman_mse'],
                'splitter': ['best','random']
            }
        }
    }
    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
    for algo_name, config in algos.items():
        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        gs.fit(X,y)
        scores.append({
            'model': algo_name,
            'best_score': gs.best_score_,
            'best_params': gs.best_params_
        })

    return pd.DataFrame(scores,columns=['model','best_score','best_params'])

import warnings
warnings.filterwarnings('ignore')

find_best_model(X,y)

"""Our best model is decision tree regressor with r-squared 0.964."""

# Ensemble models:
from sklearn.ensemble import RandomForestRegressor
from sklearn import ensemble
def ensemble_models(X_train, y_train, X_test, y_test):   
    print("RandomForestRegressor:")
    rf=RandomForestRegressor(n_estimators=20,random_state=0)
    rf.fit(X_train,y_train)
    print(rf.score(X_test, y_test))
    
    print("Gradient Boosting:")
    gb=ensemble.GradientBoostingRegressor(n_estimators=200,max_depth=5)
    gb.fit(X_train,y_train)
    print(gb.score(X_test, y_test))
    
    return [rf,gb]

ensemble_models(X_train, y_train, X_test, y_test)