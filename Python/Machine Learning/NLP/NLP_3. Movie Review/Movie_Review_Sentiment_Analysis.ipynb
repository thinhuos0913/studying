{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie_Review_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ltGHQdPZ3U2",
        "outputId": "01e40ff6-0bf5-402a-e621-3f09f1a02f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "filename = \"txt_sentoken.zip\"\n",
        "with ZipFile(filename, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "TCKjzT8Ka3ZQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "f-m_PMFObLd9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "skVX8TsYbYaP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "ebVX4I16bc23"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "  # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "  return lines"
      ],
      "metadata": {
        "id": "_mGBvfllbhQf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "  pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels"
      ],
      "metadata": {
        "id": "YlfAuCVXb5pP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "pckF8nAsb_TM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocuKp19YcFfD",
        "outputId": "1237441d-adb7-4741-bc6b-cf4cb779789c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrLcCymLc2zw",
        "outputId": "12b6a315-2f23-48e5-d8ba-7affedc47f49"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1800, 14781) (200, 14781)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = Xtest.shape[1]"
      ],
      "metadata": {
        "id": "yLBOcpizc9us"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKQLKFz9dNlY",
        "outputId": "351fd4a7-7f73-4b54-ec85-20f1428a5a68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14781"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def define_model(n_words):\n",
        "  # define network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yHN3pYAudPY_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtTlIlIRdYQf",
        "outputId": "55ae0d0d-d368-4330-8d35-7b65522c9655"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 50)                739100    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 739,151\n",
            "Trainable params: 739,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')"
      ],
      "metadata": {
        "id": "ptPtB04jd51z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ytrain = np.array(ytrain)\n",
        "ytest = np.array(ytest)"
      ],
      "metadata": {
        "id": "VmAuoZiKgHt_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVxUYKZidqqG",
        "outputId": "c1622f7d-e2fd-4b34-928c-72acab28b647"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "57/57 - 1s - loss: 0.6913 - accuracy: 0.6189 - 1s/epoch - 20ms/step\n",
            "Epoch 2/10\n",
            "57/57 - 0s - loss: 0.6803 - accuracy: 0.8311 - 472ms/epoch - 8ms/step\n",
            "Epoch 3/10\n",
            "57/57 - 0s - loss: 0.6601 - accuracy: 0.8206 - 468ms/epoch - 8ms/step\n",
            "Epoch 4/10\n",
            "57/57 - 0s - loss: 0.6292 - accuracy: 0.8967 - 465ms/epoch - 8ms/step\n",
            "Epoch 5/10\n",
            "57/57 - 0s - loss: 0.5879 - accuracy: 0.9306 - 482ms/epoch - 8ms/step\n",
            "Epoch 6/10\n",
            "57/57 - 0s - loss: 0.5452 - accuracy: 0.9206 - 490ms/epoch - 9ms/step\n",
            "Epoch 7/10\n",
            "57/57 - 0s - loss: 0.4965 - accuracy: 0.9394 - 481ms/epoch - 8ms/step\n",
            "Epoch 8/10\n",
            "57/57 - 1s - loss: 0.4506 - accuracy: 0.9489 - 519ms/epoch - 9ms/step\n",
            "Epoch 9/10\n",
            "57/57 - 0s - loss: 0.4068 - accuracy: 0.9511 - 481ms/epoch - 8ms/step\n",
            "Epoch 10/10\n",
            "57/57 - 0s - loss: 0.3673 - accuracy: 0.9578 - 468ms/epoch - 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff12b7f2650>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5piSerZgooN",
        "outputId": "d53a52ff-b950-485d-f6c2-c5a136da5416"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 88.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict new review\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "  # clean\n",
        "  tokens = clean_doc(review)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  # convert to line\n",
        "  line = ' '.join(tokens)\n",
        "  # encode\n",
        "  encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(encoded, verbose=0)\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'"
      ],
      "metadata": {
        "id": "-daVdmKMhBxr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# positive review\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# negative review\n",
        "text = 'This is a bad movie, not recommend'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxo0ETmWhMzg",
        "outputId": "88419d3c-6f1e-44f8-c23b-c9a3961c5375"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: [Best movie ever! It was great, I recommend it.]\n",
            "Sentiment: POSITIVE (99.999%)\n",
            "Review: [This is a bad movie, not recommend]\n",
            "Sentiment: NEGATIVE (99.971%)\n"
          ]
        }
      ]
    }
  ]
}